{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a834c945-f2cf-43a0-94a9-edf28d6a8c3f",
   "metadata": {},
   "source": [
    "## Training Script\n",
    "\n",
    "This notebook serves as a reference for SFT for following 9 indian languages + english language.\n",
    "\n",
    "1. Hindi\n",
    "2. Telugu\n",
    "3. Tamil\n",
    "4. Kannada\n",
    "5. Malayalam\n",
    "6. Gujarati\n",
    "7. Odia\n",
    "8. Bengali\n",
    "9. Punjabi\n",
    "10. English\n",
    "\n",
    "We use [unsloth](https://github.com/unslothai/unsloth) library for fine-tuning the `gemma-7b` model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f32e689-beab-4cc9-8864-15f58fac0fe9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbeaefa-5b83-469f-81bb-9922a1293e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"unsloth[colab-ampere] @ git+https://github.com/unslothai/unsloth.git\" wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73257035-1a0f-4055-8ef9-3df3dadcc14d",
   "metadata": {},
   "source": [
    "### wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db0a2b2-c01c-4342-bd51-6a792f75a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login '<wandb api key>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b6bb43-69ce-4d84-ac8e-9f17e8533843",
   "metadata": {},
   "source": [
    "### HuggingFace login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f2385f-959b-4e63-aee3-4d332f0909ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# This will prompt you to enter your token\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b891c5a3-0f9a-4864-b1d8-40a6fb2b5a76",
   "metadata": {},
   "source": [
    "### Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e6f8c-6faa-4daf-a312-75eadcc525fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use for 4bit quantization\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"google/gemma-7b\", \n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = '<HF_TOKEN>',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db745722-43de-49e2-9c8f-2eb6d0b18a8e",
   "metadata": {},
   "source": [
    "### LoRA setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e26eb1e-c9a2-44c6-817f-6d0b9b596e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 128,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894a305b-46eb-47a7-9cc0-3966714e97e8",
   "metadata": {},
   "source": [
    "### Test tokens for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611b9c50-479b-47ff-b11e-eee7f4790964",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google News is a news aggregator service developed by Google. It presents a continuous flow of links to articles organized from thousands of publishers and magazines. Google News is available as an app on Android, iOS, and the Web. Google released a beta version in September 2002 and the official app in January 2006.\n"
     ]
    }
   ],
   "source": [
    "# English reference text\n",
    "\n",
    "text = \"\"\"Google News is a news aggregator service developed by Google. It presents a continuous flow of links to articles organized from thousands of publishers and magazines. Google News is available as an app on Android, iOS, and the Web. Google released a beta version in September 2002 and the official app in January 2006.\"\"\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc88604-bec4-4e37-a147-2722e70c2072",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_text = \"\"\"Google News Google द्वारा विकसित एक समाचार एग्रीगेटर सेवा है। यह हजारों प्रकाशकों और पत्रिकाओं से व्यवस्थित लेखों के लिंक का निरंतर प्रवाह प्रस्तुत करता है। Google News एंड्रॉइड, iOS और वेब पर एक ऐप के रूप में उपलब्ध है। Google ने सितंबर 2002 में बीटा संस्करण और जनवरी 2006 में आधिकारिक ऐप जारी किया।\"\"\"\n",
    "tamil_text = \"\"\"கூகுள் செய்திகள் என்பது கூகுள் உருவாக்கிய செய்தி சேகரிப்பு சேவையாகும். ஆயிரக்கணக்கான வெளியீட்டாளர்கள் மற்றும் பத்திரிகைகளிலிருந்து ஒழுங்கமைக்கப்பட்ட கட்டுரைகளுக்கான தொடர்ச்சியான இணைப்புகளை இது வழங்குகிறது. Android, iOS மற்றும் இணையத்தில் Google செய்திகள் ஒரு பயன்பாடாகக் கிடைக்கிறது. கூகுள் செப்டம்பர் 2002 இல் பீட்டா பதிப்பையும், ஜனவரி 2006 இல் அதிகாரப்பூர்வ பயன்பாட்டையும் வெளியிட்டது.\"\"\"\n",
    "malayalam_text = \"\"\"Google വികസിപ്പിച്ച വാർത്താ അഗ്രഗേറ്റർ സേവനമാണ് Google News. ആയിരക്കണക്കിന് പ്രസാധകരിൽ നിന്നും മാസികകളിൽ നിന്നും സംഘടിപ്പിച്ച ലേഖനങ്ങളിലേക്കുള്ള ലിങ്കുകളുടെ തുടർച്ചയായ ഒഴുക്ക് ഇത് അവതരിപ്പിക്കുന്നു. Android, iOS, Web എന്നിവയിൽ Google News ഒരു ആപ്പായി ലഭ്യമാണ്. ഗൂഗിൾ 2002 സെപ്റ്റംബറിൽ ഒരു ബീറ്റ പതിപ്പും 2006 ജനുവരിയിൽ ഔദ്യോഗിക ആപ്പും പുറത്തിറക്കി.\"\"\"\n",
    "gujarati_text = \"\"\"ગૂગલ ન્યૂઝ એ ગૂગલ દ્વારા વિકસિત ન્યૂઝ એગ્રીગેટર સેવા છે. તે હજારો પ્રકાશકો અને સામયિકોમાંથી આયોજિત લેખોની લિંક્સનો સતત પ્રવાહ રજૂ કરે છે. Google News એ Android, iOS અને વેબ પર એપ્લિકેશન તરીકે ઉપલબ્ધ છે. ગૂગલે સપ્ટેમ્બર 2002માં બીટા વર્ઝન અને જાન્યુઆરી 2006માં સત્તાવાર એપ બહાર પાડી.\"\"\"\n",
    "bengali_text = \"\"\"গুগল নিউজ হল একটি নিউজ এগ্রিগেটর সার্ভিস যা গুগল ডেভেলপ করেছে। এটি হাজার হাজার প্রকাশক এবং ম্যাগাজিন থেকে সংগঠিত নিবন্ধগুলির লিঙ্কগুলির একটি অবিচ্ছিন্ন প্রবাহ উপস্থাপন করে। Google News Android, iOS এবং ওয়েবে একটি অ্যাপ হিসেবে উপলব্ধ। গুগল 2002 সালের সেপ্টেম্বরে একটি বিটা সংস্করণ এবং জানুয়ারী 2006 সালে অফিসিয়াল অ্যাপ প্রকাশ করে।\"\"\"\n",
    "odia_text = \"\"\"ଗୁଲ୍ ଦ୍ୱାରା ବିକଶିତ ଏକ ନ୍ୟୁଜ୍ ଏଗ୍ରିଗେଟର୍ ସେବା | ଏହା ହଜାର ହଜାର ପ୍ରକାଶକ ଏବଂ ପତ୍ରିକାଗୁଡ଼ିକରୁ ସଂଗଠିତ ପ୍ରବନ୍ଧଗୁଡିକ ସହିତ ଲିଙ୍କଗୁଡିକର କ୍ରମାଗତ ପ୍ରବାହକୁ ଉପସ୍ଥାପନ କରେ | ଗୁଗୁଲ୍ ନ୍ୟୁଜ୍ ଆଣ୍ଡ୍ରଏଡ୍, ଆଇଓଏସ୍ ଏବଂ ୱେବରେ ଏକ ଆପ୍ ଭାବରେ ଉପଲବ୍ଧ | ସେପ୍ଟେମ୍ବର 2002 ରେ ଗୁଗୁଲ୍ ଏକ ବିଟା ସଂସ୍କରଣ ଏବଂ ଜାନୁଆରୀ 2006 ରେ ଅଫିସିଆଲ୍ ଆପ୍ ପ୍ରକାଶ କରିଥିଲା ​​|\"\"\"\n",
    "telugu_text = \"\"\"Google వార్తలు అనేది Google ద్వారా అభివృద్ధి చేయబడిన వార్తా అగ్రిగేటర్ సేవ. ఇది వేలకొద్దీ ప్రచురణకర్తలు మరియు మ్యాగజైన్‌ల నుండి నిర్వహించబడిన కథనాలకు నిరంతర లింక్‌లను అందిస్తుంది. Google వార్తలు Android, iOS మరియు వెబ్‌లో యాప్‌గా అందుబాటులో ఉన్నాయి. గూగుల్ సెప్టెంబరు 2002లో బీటా వెర్షన్‌ను మరియు జనవరి 2006లో అధికారిక యాప్‌ను విడుదల చేసింది.\"\"\"\n",
    "punjabi_text = \"\"\"ਗੂਗਲ ਨਿਊਜ਼ ਗੂਗਲ ਦੁਆਰਾ ਵਿਕਸਿਤ ਕੀਤੀ ਗਈ ਇੱਕ ਨਿਊਜ਼ ਐਗਰੀਗੇਟਰ ਸੇਵਾ ਹੈ। ਇਹ ਹਜ਼ਾਰਾਂ ਪ੍ਰਕਾਸ਼ਕਾਂ ਅਤੇ ਰਸਾਲਿਆਂ ਤੋਂ ਸੰਗਠਿਤ ਲੇਖਾਂ ਦੇ ਲਿੰਕਾਂ ਦਾ ਨਿਰੰਤਰ ਪ੍ਰਵਾਹ ਪੇਸ਼ ਕਰਦਾ ਹੈ। Google News Android, iOS ਅਤੇ ਵੈੱਬ 'ਤੇ ਇੱਕ ਐਪ ਵਜੋਂ ਉਪਲਬਧ ਹੈ। ਗੂਗਲ ਨੇ ਸਤੰਬਰ 2002 ਵਿੱਚ ਇੱਕ ਬੀਟਾ ਸੰਸਕਰਣ ਅਤੇ ਜਨਵਰੀ 2006 ਵਿੱਚ ਅਧਿਕਾਰਤ ਐਪ ਜਾਰੀ ਕੀਤਾ।\"\"\"\n",
    "kannada_text = \"\"\"Google News Google ನಿಂದ ಅಭಿವೃದ್ಧಿಪಡಿಸಲಾದ ಸುದ್ದಿ ಸಂಗ್ರಾಹಕ ಸೇವೆಯಾಗಿದೆ. ಇದು ಸಾವಿರಾರು ಪ್ರಕಾಶಕರು ಮತ್ತು ನಿಯತಕಾಲಿಕೆಗಳಿಂದ ಆಯೋಜಿಸಲಾದ ಲೇಖನಗಳಿಗೆ ನಿರಂತರವಾದ ಲಿಂಕ್‌ಗಳನ್ನು ಪ್ರಸ್ತುತಪಡಿಸುತ್ತದೆ. Android, iOS ಮತ್ತು ವೆಬ್‌ನಲ್ಲಿ Google News ಅಪ್ಲಿಕೇಶನ್‌ನಂತೆ ಲಭ್ಯವಿದೆ. ಗೂಗಲ್ ಸೆಪ್ಟೆಂಬರ್ 2002 ರಲ್ಲಿ ಬೀಟಾ ಆವೃತ್ತಿಯನ್ನು ಮತ್ತು ಜನವರಿ 2006 ರಲ್ಲಿ ಅಧಿಕೃತ ಅಪ್ಲಿಕೇಶನ್ ಅನ್ನು ಬಿಡುಗಡೆ ಮಾಡಿತು.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4fa6d1-6c17-4bb3-8c2d-49c0cab8f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_text = {\n",
    "         'hindi': hindi_text, \n",
    "         'tamil': tamil_text, \n",
    "         'malayalam': malayalam_text, \n",
    "         'gujarati': gujarati_text, \n",
    "         'bengali': bengali_text, \n",
    "         'odia': odia_text, \n",
    "         'telugu': telugu_text, \n",
    "         'punjabi': punjabi_text, \n",
    "         'kannada': kannada_text\n",
    "        }\n",
    "\n",
    "for language in language_text:\n",
    "    print(\"Language: \", language)\n",
    "    print(tokenizer.tokenize(language_text[language][:50]))\n",
    "    print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f0c321-75bb-47bf-b592-23475d1898b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc4179-f3da-4c35-8224-e595d2abf627",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c808b3-b649-416a-b7b3-56bd2282f58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7597f8-2cc3-472a-b3f1-0cb98c39d316",
   "metadata": {},
   "source": [
    "### EOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c732ac0c-f15c-4745-8c1a-289b2361dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1e2919-ea4e-4af8-9877-58e37446f2e7",
   "metadata": {},
   "source": [
    "### Prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066abd09-2e12-44cc-915e-0706250c32ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        if input is None:\n",
    "            input = \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "def formatting_prompts_func_malayalam(examples):\n",
    "    data = examples[\"Prompt\"]\n",
    "    texts = []\n",
    "    for text in data:\n",
    "        if text is None:\n",
    "            pass\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = text + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "def formatting_prompts_func_telugu(examples):\n",
    "    instructions = examples[\"telugu_instruction\"]\n",
    "    inputs       = examples[\"telugu_input\"]\n",
    "    outputs      = examples[\"telugu_output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        if input is None:\n",
    "            input = \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472f531e-043b-4ef0-9dd8-3af38f5324b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset_name, \n",
    "                    split_type, \n",
    "                    processing_func, \n",
    "                    rename_column = False, \n",
    "                    filter_data = False, \n",
    "                    filter_column_value = 'id', \n",
    "                    filter_value = 'alpaca',\n",
    "                    num_samples=20000):\n",
    "    \n",
    "    if isinstance(dataset_name, str):\n",
    "        dataset = load_dataset(dataset_name, split=split_type)\n",
    "    else:\n",
    "        # Assuming dataset_name is a filepath for JSON file\n",
    "        with open(dataset_name, 'r') as file:\n",
    "            data = []\n",
    "            for line_number, line in enumerate(file, 1):\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error parsing JSON at line {line_number}: {e}\")\n",
    "            dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
    "    \n",
    "    if post_processing_steps:\n",
    "        for post_processing_step in post_processing_steps:\n",
    "            dataset = post_processing_step(dataset)\n",
    "    \n",
    "    if rename_column:\n",
    "        dataset = rename(dataset)\n",
    "    \n",
    "    if filter_data:\n",
    "        dataset = filter_dataset(dataset, num_samples, value, column_name)\n",
    "    \n",
    "    dataset = dataset.map(processing_func, batched=True)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Define the additional processing steps\n",
    "def rename(dataset):\n",
    "    return dataset.rename_column('response', 'output')\n",
    "\n",
    "def filter_dataset(dataset, num_samples, value, column_name):\n",
    "    return dataset.filter(lambda example: value in example[column_name]).shuffle(seed=42).select(range(num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239607db-365f-4d3c-a10b-583c466f1d54",
   "metadata": {},
   "source": [
    "### Process each language dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b6c6d0-1716-4802-a660-79f9dcd37968",
   "metadata": {},
   "source": [
    "#### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d680965-5d5e-40bd-9a67-34d7893dff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_en = process_dataset(\"yahma/alpaca-cleaned\", \"train\", formatting_prompts_func)\n",
    "\n",
    "print(dataset_en['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf6c13-b027-4348-9d33-f6f02752bf4d",
   "metadata": {},
   "source": [
    "#### Gujarati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1b8b92-1a9e-4f4d-8b52-7e5ffbd23397",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_guj = process_dataset('gujarati.json', \"train\", formatting_prompts_func)\n",
    "print(dataset_guj['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588427c0-22a7-4156-9e49-76b9db7a6a87",
   "metadata": {},
   "source": [
    "#### Kannada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e119e71-fddd-44ad-a1d4-0c8147203c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_kn1 = process_dataset(\"Tensoic/airoboros-3.2_kn\", \"train\", formatting_prompts_func)\n",
    "\n",
    "dataset_kn2 = process_dataset(\"Tensoic/gpt-teacher_kn\", \"train\", formatting_prompts_func, True)\n",
    "\n",
    "print(dataset_kn1['text'][0])\n",
    "\n",
    "print(dataset_kn2['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27041c-df4d-45eb-a20e-41e6033d2281",
   "metadata": {},
   "source": [
    "#### Malayalam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86c4f7d-c3fa-4437-8f61-df5cee1baff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_mal = process_dataset(\"VishnuPJ/Alpaca_Instruct_Malayalam\", \"train\", formatting_prompts_func_malayalam)\n",
    "\n",
    "print(dataset_mal['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf9caa7-1ded-415e-970b-5485bfea99c0",
   "metadata": {},
   "source": [
    "#### Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd8aee-9e80-4aa6-bb92-688831f7bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hi1 = process_dataset(\"ravithejads/samvaad-hi-filtered\", \"train\", formatting_prompts_func)\n",
    "\n",
    "dataset_hi2 = process_dataset(\"HydraIndicLM/hindi_alpaca_dolly_67k\", \"train\", formatting_prompts_func, filter_data = True)\n",
    "\n",
    "print(dataset_hi1['text'][0])\n",
    "\n",
    "print(dataset_hi2['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7cf03d-9a96-43e3-8f02-c524a07e0baf",
   "metadata": {},
   "source": [
    "#### Odia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399dbd2-2dbc-438d-b8a7-802301555ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_odi1 = process_dataset(\"OdiaGenAI/Odia_Alpaca_instructions_52k\", \"train\", formatting_prompts_func)\n",
    "\n",
    "dataset_odi2 = process_dataset(\"OdiaGenAI/gpt-teacher-roleplay-odia-3k\", \"train\", formatting_prompts_func, rename = True)\n",
    "\n",
    "print(dataset_odi1['text'][0])\n",
    "\n",
    "print(dataset_odi2['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1cb817-f266-4405-b0a1-37b92c4e7e5d",
   "metadata": {},
   "source": [
    "#### Punjabi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf611d-db41-4fc5-a86d-05e1567a1534",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pun = process_dataset(\"HydraIndicLM/punjabi_alpaca_52K\", \"train\", formatting_prompts_func)\n",
    "\n",
    "print(dataset_pun['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f75f63-eedd-448b-badc-e52eb770eb25",
   "metadata": {},
   "source": [
    "#### Bengali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb75f60e-7f26-43e9-a915-707d76ee38cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ben = process_dataset(\"HydraIndicLM/bengali_alpaca_dolly_67k\", \"train\", formatting_prompts_func, filter_data = True)\n",
    "\n",
    "print(dataset_ben['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2255f4-a16b-4c40-8618-c49423007897",
   "metadata": {},
   "source": [
    "#### Tamil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0364e769-1321-49a7-b6ac-6488c7c339b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Language: \")\n",
    "\n",
    "dataset_ta = process_dataset(\"abhinand/tamil-alpaca\", \"train\", formatting_prompts_func)\n",
    "\n",
    "print(dataset_ta['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed007b33-0d4e-446e-bdd0-b75db4149bc5",
   "metadata": {},
   "source": [
    "#### Telugu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76df52-d1ab-47be-99ac-c7a181c6518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_te1 = process_dataset(\"Telugu-LLM-Labs/yahma_alpaca_cleaned_telugu_filtered_and_romanized\", \"train\", formatting_prompts_func_telugu)\n",
    "\n",
    "dataset_te2 = process_dataset(\"Telugu-LLM-Labs/teknium_GPTeacher_general_instruct_telugu_filtered_and_romanized\", \"train\", formatting_prompts_func_telugu)\n",
    "\n",
    "print(dataset_te1['text'][0])\n",
    "\n",
    "print(dataset_te2['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df30d9-f7db-4d1a-8ac6-542adaa44219",
   "metadata": {},
   "source": [
    "### Final dataset concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d0c5b1-39a5-417a-a58b-2e5633e95e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([\n",
    "    dataset_en, dataset_guj, dataset_kn1, dataset_kn2, dataset_mal, dataset_hi1, dataset_hi2,\n",
    "    dataset_pun, dataset_ben, dataset_ta, dataset_te1, dataset_te2, dataset_odi1, dataset_odi2\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29007d94-140b-4e7c-890d-7101d09bd527",
   "metadata": {},
   "source": [
    "### Shuffle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a91f203-660f-46fc-a47d-42f0ce97ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['text'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ddfb3c-7dba-439d-9e25-104352937899",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sft = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b6a80-ac88-4350-82ff-aebb8802f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_sft['text'][100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1108fa-2d40-4630-96e4-f4a9ce529877",
   "metadata": {},
   "source": [
    "### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faac191-adc1-4559-8d01-4725501f5e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(dataset_sft['text'][100])\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd73dd58-814b-48e1-8b6c-4727dd68a5bc",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43e112b-4bac-4a95-a9ab-b2e54947677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 2\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 8\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 8\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 8\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 10\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency and make training 5x faster for short sequences.\n",
    "packing = True\n",
    "\n",
    "# text field in dataset\n",
    "dataset_text_field = \"text\"\n",
    "\n",
    "# dataset para,\n",
    "dataset_num_proc = 2\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "# device_map = {\"\": 0}\n",
    "device_map = \"auto\"\n",
    "\n",
    "# monitoring\n",
    "report_to = \"wandb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb7bce-fd16-4825-be63-9dcec27dbea7",
   "metadata": {},
   "source": [
    "### TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505ff039-b0f1-412a-a1b8-e840c60655c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=report_to,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f883febf-b245-41aa-a734-8c12dec59208",
   "metadata": {},
   "source": [
    "### SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2765897-458a-4652-b26b-b03419f208fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset_sft,\n",
    "    dataset_text_field = dataset_text_field,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = dataset_num_proc,\n",
    "    packing = packing,\n",
    "    args = training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aeace4-d76b-4cf6-9e1d-9877ff1c10d2",
   "metadata": {},
   "source": [
    "### Current memory stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b9ac44-1811-41f7-8cb1-1443dc5570fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350e847-6572-4d31-9983-8e04a1da8e0a",
   "metadata": {},
   "source": [
    "### Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ec40a-de62-4e81-8a47-133aaf932f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"<project>\", name=\"<name>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041748c4-fe84-4c8d-9827-e6744a1d3b07",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a1b0cb-b296-4239-9bd9-948f2bcf4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615444f4-a5e0-4b7a-845c-3d483111d940",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886026cc-e132-41d8-8d2e-13b12f0ae3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"<FINE_TUNED_MODEL_NAME>\") # Local saving\n",
    "model.push_to_hub(\"<HF_REPO_NAME>/<FINE_TUNED_MODEL_NAME>\", token = '<HF_WRITE_TOKEN>') \n",
    "tokenizer.push_to_hub(\"<HF_REPO_NAME>/<FINE_TUNED_MODEL_NAME>\", token = '<HF_WRITE_TOKEN>') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f969474-9740-4a2a-a185-3b29dee77a2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f144e6e-9ca4-4ae6-96e3-b9069a6611a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"(9+1)+(5+0). इसे 3 चरणों में हल करें.\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 300, use_cache = True)\n",
    "print(tokenizer.batch_decode(outputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
